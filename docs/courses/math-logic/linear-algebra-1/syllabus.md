# Linear Algebra I

**Pillar:** Math & Logic  
**Purpose:** The algebra of vectors, matrices, and transformations—the backbone of data science, optimization, signals, control, and finance.

## Learning Outcomes
1. Solve linear systems; interpret solution sets and rank.
2. Perform matrix operations; understand linear transformations and change of basis.
3. Compute eigenvalues/eigenvectors; interpret diagonalization.
4. Apply least squares and projections to modeling/data.
5. Communicate geometric intuition (subspaces, dimension, orthogonality).

## Modules & Weekly Topics
1. **Vectors & Systems** — linear combinations, span, Gaussian elimination.
2. **Matrices & Transformations** — composition, inverses, determinants (intuition).
3. **Orthogonality & Least Squares** — inner products, projections, normal equations.
4. **Eigenstuff** — eigenvalues/eigenvectors, diagonalization, applications.
5. **Applications** — Markov chains, PageRank, PCA intuition.
6. **Integration** — mini-capstone: model a small dataset with least squares & interpret.

## Resources
- **Primary:** Strang, *Introduction to Linear Algebra*.
- **Supplemental:** 3Blue1Brown, *Essence of Linear Algebra* videos.
- **Tooling:** Python (NumPy), Jupyter.

## Assignments & Evaluation
- **Problem Sets:** 6 sets (~160 problems).
- **Project:** PCA or Markov chain mini-project (5–7 pages + code).
- **Integration:** 1-page memo: where this appears in OR/controls/finance.
- **Weighting:** PS 55%, Project 35%, Integration 10%.

## Portfolio Output
- Report, code, and memo saved in `portfolio/`.
